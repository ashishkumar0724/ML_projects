{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install ebooklib nltk tqdm\n",
        "\n",
        "# Import all necessary libraries\n",
        "import os\n",
        "import re\n",
        "import pickle\n",
        "import string\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import torch.nn.functional as F\n",
        "from ebooklib import epub\n",
        "import ebooklib\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_p9tOW8jX7XL",
        "outputId": "1bd2840d-3b9f-4bca-e8b5-9ae756e586c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ebooklib in /usr/local/lib/python3.11/dist-packages (0.19)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from ebooklib) (5.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from ebooklib) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Upload EPUB file\n",
        "print(\"Please upload your EPUB file:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Get the uploaded filename\n",
        "epub_filename = list(uploaded.keys())[0]\n",
        "epub_path = f\"/content/{epub_filename}\"\n",
        "print(f\"Uploaded file: {epub_filename}\")\n",
        "\n",
        "# Set up NLTK data\n",
        "nltk.download('punkt', quiet=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "cx3VNGVYZrN2",
        "outputId": "cdf34f2f-7825-46f7-a8e5-685eb9334703"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please upload your EPUB file:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a0e9b9bb-5e38-462b-aa5a-19ee6555f575\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a0e9b9bb-5e38-462b-aa5a-19ee6555f575\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving linux.epub to linux.epub\n",
            "Uploaded file: linux.epub\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_epub(epub_path):\n",
        "    \"\"\"Extract clean text from EPUB file\"\"\"\n",
        "    try:\n",
        "        book = epub.read_epub(epub_path)\n",
        "        text = \"\"\n",
        "        for item in book.get_items():\n",
        "            if item.get_type() == ebooklib.ITEM_DOCUMENT:\n",
        "                try:\n",
        "                    raw_text = item.get_content().decode('utf-8')\n",
        "                    # Remove HTML tags but preserve text structure\n",
        "                    clean_text = re.sub('<[^<]+?>', '', raw_text)\n",
        "                    text += clean_text + \"\\n\"\n",
        "                except Exception as e:\n",
        "                    print(f\"Warning: Could not decode item: {e}\")\n",
        "                    continue\n",
        "        return text.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading EPUB: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Light preprocessing to preserve character-level information\"\"\"\n",
        "    # Normalize whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    # Remove excessive newlines but keep some structure\n",
        "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
        "    return text.strip()\n",
        "\n",
        "# Extract and preprocess text\n",
        "print(\"Extracting text from EPUB...\")\n",
        "raw_text = extract_text_from_epub(epub_path)\n",
        "cleaned_text = preprocess_text(raw_text)\n",
        "\n",
        "print(f\"Text extracted: {len(cleaned_text)} characters\")\n",
        "print(\"Sample text:\")\n",
        "print(cleaned_text[:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqT4zlrUZ0Y3",
        "outputId": "8b7af2b7-fd90-4e38-8845-c0904aec3eb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting text from EPUB...\n",
            "Text extracted: 429330 characters\n",
            "Sample text:\n",
            "&#13; Contents in Detail&#13; &#13; Cover Page&#13; Title Page&#13; Copyright Page&#13; Dedication&#13; About the Author&#13; About the Technical Reviewer&#13; BRIEF CONTENTS&#13; CONTENTS IN DETAIL&#13; ACKNOWLEDGMENTS&#13; INTRODUCTION&#13; &#13; What’s in This Book&#13; What Is Ethical Hacking?&#13; Why Hackers Use Linux&#13; Downloading Kali Linux&#13; Virtual Machines&#13; Setting Up Kali&#13; &#13; 1 GETTING STARTED WITH THE BASICS&#13; &#13; Introductory Terms and Concepts&#13; A Tour of \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class OptimizedBookDataset(Dataset):\n",
        "    def __init__(self, text, seq_length):\n",
        "        self.text = text\n",
        "        self.seq_length = seq_length\n",
        "        # Create vocabulary\n",
        "        self.vocab = sorted(set(text))\n",
        "        self.char_to_idx = {char: idx for idx, char in enumerate(self.vocab)}\n",
        "        self.idx_to_char = {idx: char for idx, char in enumerate(self.vocab)}\n",
        "        self.vocab_size = len(self.vocab)\n",
        "        print(f\"Vocabulary size: {self.vocab_size}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return max(0, len(self.text) - self.seq_length)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Generate sequences on-the-fly to save memory\n",
        "        chunk = self.text[idx:idx + self.seq_length + 1]\n",
        "        if len(chunk) < self.seq_length + 1:\n",
        "            # Pad if chunk is too short\n",
        "            chunk = chunk + ' ' * (self.seq_length + 1 - len(chunk))\n",
        "\n",
        "        encoded = [self.char_to_idx.get(char, 0) for char in chunk]\n",
        "        return torch.tensor(encoded[:-1], dtype=torch.long), torch.tensor(encoded[1:], dtype=torch.long)\n",
        "\n",
        "# Create dataset\n",
        "SEQ_LENGTH = 50  # Reduced for faster training\n",
        "print(\"Creating dataset...\")\n",
        "dataset = OptimizedBookDataset(cleaned_text, SEQ_LENGTH)\n",
        "\n",
        "# Save dataset for later use\n",
        "dataset_save_path = \"/content/book_dataset.pkl\"\n",
        "with open(dataset_save_path, \"wb\") as f:\n",
        "    pickle.dump({\n",
        "        'vocab': dataset.vocab,\n",
        "        'char_to_idx': dataset.char_to_idx,\n",
        "        'idx_to_char': dataset.idx_to_char,\n",
        "        'text': cleaned_text,\n",
        "        'seq_length': SEQ_LENGTH\n",
        "    }, f)\n",
        "print(f\"Dataset saved to {dataset_save_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvls820mZ2ui",
        "outputId": "de6784db-374f-4e88-ca08-c6b803e80093"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating dataset...\n",
            "Vocabulary size: 109\n",
            "Dataset saved to /content/book_dataset.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class OptimizedTextGenerator(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=256, num_layers=2, dropout=0.3):\n",
        "        super(OptimizedTextGenerator, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # LSTM with dropout\n",
        "        self.lstm = nn.LSTM(\n",
        "            embedding_dim,\n",
        "            hidden_dim,\n",
        "            num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0,\n",
        "            bidirectional=False\n",
        "        )\n",
        "\n",
        "        # Dropout and output layer\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        # Embedding\n",
        "        embedded = self.embedding(x)  # (batch, seq_len) -> (batch, seq_len, embedding_dim)\n",
        "\n",
        "        # LSTM\n",
        "        lstm_out, hidden = self.lstm(embedded, hidden)\n",
        "\n",
        "        # Dropout\n",
        "        lstm_out = self.dropout(lstm_out)\n",
        "\n",
        "        # Output projection\n",
        "        output = self.fc(lstm_out)\n",
        "\n",
        "        return output, hidden\n",
        "\n",
        "# Initialize model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model = OptimizedTextGenerator(\n",
        "    vocab_size=len(dataset.vocab),\n",
        "    embedding_dim=128,\n",
        "    hidden_dim=256,\n",
        "    num_layers=2,\n",
        "    dropout=0.3\n",
        ").to(device)\n",
        "\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XtPybSMaWZ-",
        "outputId": "9ff12306-f459-42b8-950d-f3171154a29e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Model parameters: 963,565\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DataLoader with optimizations\n",
        "BATCH_SIZE = 64  # Increased for better GPU utilization\n",
        "dataloader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True,\n",
        "    persistent_workers=True\n",
        ")\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
        "scaler = GradScaler()  # For mixed precision training\n",
        "\n",
        "print(f\"Number of batches: {len(dataloader)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggItGRAnaZa0",
        "outputId": "f9ed86ad-181f-4b88-a3a3-deb7790069f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of batches: 6708\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2429895055.py:16: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()  # For mixed precision training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, dataloader, num_epochs=10):\n",
        "    \"\"\"Optimized training function with fixed autocast\"\"\"\n",
        "\n",
        "    # Training history\n",
        "    train_losses = []\n",
        "\n",
        "    # Determine device and autocast dtype\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device('cuda')\n",
        "        # Use autocast for GPU\n",
        "        from torch.amp import autocast\n",
        "    else:\n",
        "        device = torch.device('cpu')\n",
        "        # CPU doesn't benefit much from autocast, so we'll use a dummy context\n",
        "        from contextlib import nullcontext\n",
        "        autocast = lambda: nullcontext()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        num_batches = 0\n",
        "\n",
        "        # Progress bar\n",
        "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        for batch_idx, (inputs, targets) in enumerate(progress_bar):\n",
        "            # Move to device\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            # Zero gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass with proper autocast\n",
        "            if torch.cuda.is_available():\n",
        "                with autocast('cuda'):\n",
        "                    outputs, _ = model(inputs)\n",
        "                    # Reshape for loss calculation\n",
        "                    outputs = outputs.reshape(-1, outputs.size(-1))\n",
        "                    targets = targets.reshape(-1)\n",
        "                    loss = criterion(outputs, targets)\n",
        "            else:\n",
        "                outputs, _ = model(inputs)\n",
        "                outputs = outputs.reshape(-1, outputs.size(-1))\n",
        "                targets = targets.reshape(-1)\n",
        "                loss = criterion(outputs, targets)\n",
        "\n",
        "            # Backward pass\n",
        "            if torch.cuda.is_available():\n",
        "                scaler.scale(loss).backward()\n",
        "\n",
        "                # Gradient clipping\n",
        "                scaler.unscale_(optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "                # Optimizer step\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "            else:\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "                optimizer.step()\n",
        "\n",
        "            # Update progress bar\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "            progress_bar.set_postfix({\n",
        "                'loss': f'{loss.item():.4f}',\n",
        "                'avg_loss': f'{total_loss/num_batches:.4f}'\n",
        "            })\n",
        "\n",
        "            # Clear cache periodically to prevent memory issues\n",
        "            if batch_idx % 500 == 0 and torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        # Calculate average loss\n",
        "        avg_loss = total_loss / num_batches\n",
        "        train_losses.append(avg_loss)\n",
        "\n",
        "        # Learning rate scheduling\n",
        "        scheduler.step(avg_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} - Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        # Save model checkpoint\n",
        "        if (epoch + 1) % 2 == 0:\n",
        "            checkpoint_path = f\"/content/model_checkpoint_epoch_{epoch+1}.pth\"\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': avg_loss,\n",
        "                'vocab': dataset.vocab,\n",
        "                'char_to_idx': dataset.char_to_idx,\n",
        "                'idx_to_char': dataset.idx_to_char\n",
        "            }, checkpoint_path)\n",
        "            print(f\"Checkpoint saved: {checkpoint_path}\")\n",
        "\n",
        "    return train_losses\n",
        "\n",
        "# Train the model\n",
        "print(\"Starting training...\")\n",
        "train_losses = train_model(model, dataloader, num_epochs=5)\n",
        "print(\"Training completed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbMBFXxfajez",
        "outputId": "291bc994-e86b-41c6-cd44-b2ce062bba67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5: 100%|██████████| 6708/6708 [01:22<00:00, 80.87it/s, loss=1.1052, avg_loss=1.3545]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5 - Average Loss: 1.3545\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5: 100%|██████████| 6708/6708 [01:19<00:00, 83.86it/s, loss=1.0418, avg_loss=1.0313]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5 - Average Loss: 1.0313\n",
            "Checkpoint saved: /content/model_checkpoint_epoch_2.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5: 100%|██████████| 6708/6708 [01:20<00:00, 82.84it/s, loss=0.9978, avg_loss=0.9635]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5 - Average Loss: 0.9635\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5: 100%|██████████| 6708/6708 [01:20<00:00, 83.83it/s, loss=0.9223, avg_loss=0.9296]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5 - Average Loss: 0.9296\n",
            "Checkpoint saved: /content/model_checkpoint_epoch_4.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5: 100%|██████████| 6708/6708 [01:20<00:00, 83.01it/s, loss=0.9487, avg_loss=0.9080]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5 - Average Loss: 0.9080\n",
            "Training completed!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Continue training for better results\n",
        "train_model(model, dataloader, num_epochs=10)  # Additional 10 epochs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LkpavQzlXIn",
        "outputId": "3a4528fc-bf8e-453b-82d5-e3fc752e6f3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10: 100%|██████████| 6708/6708 [01:21<00:00, 81.99it/s, loss=0.8998, avg_loss=0.8925]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 - Average Loss: 0.8925\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/10: 100%|██████████| 6708/6708 [01:21<00:00, 82.33it/s, loss=0.8862, avg_loss=0.8801]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/10 - Average Loss: 0.8801\n",
            "Checkpoint saved: /content/model_checkpoint_epoch_2.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/10: 100%|██████████| 6708/6708 [01:19<00:00, 84.15it/s, loss=0.8505, avg_loss=0.8700]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/10 - Average Loss: 0.8700\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/10: 100%|██████████| 6708/6708 [01:21<00:00, 81.91it/s, loss=0.8854, avg_loss=0.8620]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/10 - Average Loss: 0.8620\n",
            "Checkpoint saved: /content/model_checkpoint_epoch_4.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/10: 100%|██████████| 6708/6708 [01:20<00:00, 83.52it/s, loss=0.8086, avg_loss=0.8549]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/10 - Average Loss: 0.8549\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/10: 100%|██████████| 6708/6708 [01:20<00:00, 83.01it/s, loss=0.8132, avg_loss=0.8486]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/10 - Average Loss: 0.8486\n",
            "Checkpoint saved: /content/model_checkpoint_epoch_6.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/10: 100%|██████████| 6708/6708 [01:20<00:00, 83.00it/s, loss=0.9134, avg_loss=0.8430]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/10 - Average Loss: 0.8430\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/10: 100%|██████████| 6708/6708 [01:21<00:00, 82.80it/s, loss=0.8348, avg_loss=0.8380]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/10 - Average Loss: 0.8380\n",
            "Checkpoint saved: /content/model_checkpoint_epoch_8.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/10: 100%|██████████| 6708/6708 [01:20<00:00, 83.16it/s, loss=0.7137, avg_loss=0.8330]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/10 - Average Loss: 0.8330\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/10: 100%|██████████| 6708/6708 [01:22<00:00, 81.28it/s, loss=0.8533, avg_loss=0.8290]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/10 - Average Loss: 0.8290\n",
            "Checkpoint saved: /content/model_checkpoint_epoch_10.pth\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.8925476255929392,\n",
              " 0.8801363064870282,\n",
              " 0.8700400313576845,\n",
              " 0.8619626557002411,\n",
              " 0.8548643312144294,\n",
              " 0.8486020285314083,\n",
              " 0.8429967897078361,\n",
              " 0.837973459328863,\n",
              " 0.8330102307887467,\n",
              " 0.8289728522140779]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the final trained model\n",
        "final_model_path = \"/content/final_text_generator.pth\"\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'vocab': dataset.vocab,\n",
        "    'char_to_idx': dataset.char_to_idx,\n",
        "    'idx_to_char': dataset.idx_to_char,\n",
        "    'seq_length': SEQ_LENGTH,\n",
        "    'embedding_dim': 128,\n",
        "    'hidden_dim': 256,\n",
        "    'num_layers': 2\n",
        "}, final_model_path)\n",
        "\n",
        "print(f\"Final model saved to {final_model_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rtq9YhNJhIgf",
        "outputId": "9c11df16-e16c-4eea-c1b2-c22388a4c73e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final model saved to /content/final_text_generator.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FIXED Block 9: Text Generation Function\n",
        "def generate_text(model, dataset, seed_text=\"The\", length=200, temperature=0.8):\n",
        "    \"\"\"Generate text using the trained model\"\"\"\n",
        "    model.eval()  # Important: set to evaluation mode\n",
        "    device = next(model.parameters()).device\n",
        "    print(f\"Using device: {device}\")\n",
        "    print(f\"Seed text: '{seed_text}'\")\n",
        "\n",
        "    # Encode seed text\n",
        "    encoded_seed = [dataset.char_to_idx.get(char, 0) for char in seed_text]\n",
        "    input_seq = torch.tensor(encoded_seed, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "    generated_text = seed_text\n",
        "    print(f\"Starting generation with seed length: {len(seed_text)}\")\n",
        "\n",
        "    hidden = None\n",
        "    with torch.no_grad():\n",
        "        for i in range(length):\n",
        "            # Get prediction\n",
        "            with autocast():\n",
        "                output, hidden = model(input_seq, hidden)\n",
        "\n",
        "            # Apply temperature scaling\n",
        "            output = output[0, -1] / temperature\n",
        "            probabilities = F.softmax(output, dim=0)\n",
        "\n",
        "            # Sample next character\n",
        "            next_char_idx = torch.multinomial(probabilities, 1).item()\n",
        "            next_char = dataset.idx_to_char[next_char_idx]\n",
        "\n",
        "            generated_text += next_char\n",
        "\n",
        "            # Update input sequence\n",
        "            input_seq = torch.cat([input_seq, torch.tensor([[next_char_idx]], device=device)], dim=1)\n",
        "            # Keep only the last seq_length characters\n",
        "            if input_seq.size(1) > dataset.seq_length:\n",
        "                input_seq = input_seq[:, -dataset.seq_length:]\n",
        "\n",
        "            # Progress indicator\n",
        "            if (i + 1) % 50 == 0:\n",
        "                print(f\"Generated {i + 1}/{length} characters\")\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "# Test with debugging\n",
        "print(\"=== Testing Text Generation ===\")\n",
        "try:\n",
        "    sample_text = generate_text(model, dataset, seed_text=\"The \", length=100, temperature=0.7)\n",
        "    print(\"\\n=== GENERATED TEXT ===\")\n",
        "    print(sample_text)\n",
        "    print(\"=== END OF GENERATION ===\")\n",
        "except Exception as e:\n",
        "    print(f\"Error in generation: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOjV9cgahUJP",
        "outputId": "bd47207d-5658-4e81-ba15-5c057e0d95a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Testing Text Generation ===\n",
            "Using device: cuda:0\n",
            "Seed text: 'The '\n",
            "Starting generation with seed length: 4\n",
            "Generated 50/100 characters\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3369287689.py:20: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated 100/100 characters\n",
            "\n",
            "=== GENERATED TEXT ===\n",
            "The nl command to see it in the /usr/bin directory. You may need to check whether reners-indivilual newf\n",
            "=== END OF GENERATION ===\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FIXED Block 10: Load Saved Dataset and Model\n",
        "def load_dataset_and_model(dataset_path, model_path=None):\n",
        "    \"\"\"Load saved dataset and optionally model\"\"\"\n",
        "    print(f\"Loading dataset from: {dataset_path}\")\n",
        "\n",
        "    # Load dataset\n",
        "    try:\n",
        "        with open(dataset_path, \"rb\") as f:\n",
        "            dataset_data = pickle.load(f)\n",
        "        print(\"Dataset loaded successfully!\")\n",
        "        print(f\"Dataset keys: {list(dataset_data.keys())}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading dataset: {e}\")\n",
        "        return None, None\n",
        "\n",
        "    # Recreate dataset object\n",
        "    class LoadedDataset(Dataset):\n",
        "        def __init__(self, data):\n",
        "            self.text = data['text']\n",
        "            self.vocab = data['vocab']\n",
        "            self.char_to_idx = data['char_to_idx']\n",
        "            self.idx_to_char = data['idx_to_char']\n",
        "            self.seq_length = data['seq_length']\n",
        "            self.vocab_size = len(self.vocab)\n",
        "            print(f\"Loaded dataset - Text length: {len(self.text)}, Vocab size: {self.vocab_size}\")\n",
        "\n",
        "        def __len__(self):\n",
        "            return max(0, len(self.text) - self.seq_length)\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            chunk = self.text[idx:idx + self.seq_length + 1]\n",
        "            if len(chunk) < self.seq_length + 1:\n",
        "                chunk = chunk + ' ' * (self.seq_length + 1 - len(chunk))\n",
        "            encoded = [self.char_to_idx.get(char, 0) for char in chunk]\n",
        "            return torch.tensor(encoded[:-1], dtype=torch.long), torch.tensor(encoded[1:], dtype=torch.long)\n",
        "\n",
        "    dataset = LoadedDataset(dataset_data)\n",
        "\n",
        "    # Load model if provided\n",
        "    model = None\n",
        "    if model_path and os.path.exists(model_path):\n",
        "        try:\n",
        "            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "            print(f\"Loading model on device: {device}\")\n",
        "            checkpoint = torch.load(model_path, map_location=device)\n",
        "\n",
        "            model = OptimizedTextGenerator(\n",
        "                vocab_size=len(dataset.vocab),\n",
        "                embedding_dim=checkpoint.get('embedding_dim', 128),\n",
        "                hidden_dim=checkpoint.get('hidden_dim', 256),\n",
        "                num_layers=checkpoint.get('num_layers', 2)\n",
        "            )\n",
        "            model.load_state_dict(checkpoint['model_state_dict'])\n",
        "            model.to(device)\n",
        "            model.eval()\n",
        "            print(\"Model loaded successfully!\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return dataset, None\n",
        "    else:\n",
        "        print(f\"Model file not found: {model_path}\")\n",
        "\n",
        "    return dataset, model\n",
        "\n",
        "# Test the loading function\n",
        "print(\"\\n=== Testing Load Functions ===\")\n",
        "dataset_path = \"/content/book_dataset.pkl\"\n",
        "model_path = \"/content/final_text_generator.pth\"\n",
        "\n",
        "print(\"Loading saved dataset and model...\")\n",
        "loaded_dataset, loaded_model = load_dataset_and_model(dataset_path, model_path)\n",
        "\n",
        "if loaded_dataset:\n",
        "    print(f\"Loaded dataset successfully! Vocab size: {len(loaded_dataset.vocab)}\")\n",
        "\n",
        "    # Test generation with loaded model\n",
        "    if loaded_model:\n",
        "        print(\"Testing generation with loaded model...\")\n",
        "        test_text = generate_text(loaded_model, loaded_dataset, \"Test: \", 50, 0.8)\n",
        "        print(\"Loaded model generation:\")\n",
        "        print(test_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3KDIxryhcV3",
        "outputId": "330447ea-26e9-473f-bffd-4d6b2cb6096c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Testing Load Functions ===\n",
            "Loading saved dataset and model...\n",
            "Loading dataset from: /content/book_dataset.pkl\n",
            "Dataset loaded successfully!\n",
            "Dataset keys: ['vocab', 'char_to_idx', 'idx_to_char', 'text', 'seq_length']\n",
            "Loaded dataset - Text length: 429330, Vocab size: 109\n",
            "Loading model on device: cuda\n",
            "Model loaded successfully!\n",
            "Loaded dataset successfully! Vocab size: 109\n",
            "Testing generation with loaded model...\n",
            "Using device: cuda:0\n",
            "Seed text: 'Test: '\n",
            "Starting generation with seed length: 6\n",
            "Generated 50/50 characters\n",
            "Loaded model generation:\n",
            "Test: Your File Foreming Ports --enable-gre --enable-mpl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3369287689.py:20: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NEW Block: Comprehensive Testing and Debugging\n",
        "def debug_model_and_dataset(model, dataset):\n",
        "    \"\"\"Debug function to check if everything is working\"\"\"\n",
        "    print(\"=== DEBUGGING MODEL AND DATASET ===\")\n",
        "\n",
        "    # Check dataset\n",
        "    print(f\"Dataset length: {len(dataset)}\")\n",
        "    print(f\"Vocabulary size: {len(dataset.vocab)}\")\n",
        "    print(f\"First 10 vocab chars: {dataset.vocab[:10]}\")\n",
        "    print(f\"Sequence length: {dataset.seq_length}\")\n",
        "\n",
        "    # Check a sample from dataset\n",
        "    if len(dataset) > 0:\n",
        "        sample_input, sample_target = dataset[0]\n",
        "        print(f\"Sample input shape: {sample_input.shape}\")\n",
        "        print(f\"Sample target shape: {sample_target.shape}\")\n",
        "        print(f\"Sample input: {sample_input[:10]}\")\n",
        "        print(f\"Sample target: {sample_target[:10]}\")\n",
        "\n",
        "    # Check model\n",
        "    print(f\"Model device: {next(model.parameters()).device}\")\n",
        "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "    # Test forward pass\n",
        "    if len(dataset) > 0:\n",
        "        test_input, _ = dataset[0]\n",
        "        test_input = test_input.unsqueeze(0).to(next(model.parameters()).device)\n",
        "        try:\n",
        "            with torch.no_grad():\n",
        "                output, hidden = model(test_input)\n",
        "                print(f\"Forward pass successful!\")\n",
        "                print(f\"Output shape: {output.shape}\")\n",
        "                print(f\"Hidden state type: {type(hidden)}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Forward pass failed: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "# Run debugging\n",
        "debug_model_and_dataset(model, dataset)\n",
        "\n",
        "# Simple generation test\n",
        "print(\"\\n=== SIMPLE GENERATION TEST ===\")\n",
        "try:\n",
        "    # Very simple test first\n",
        "    simple_text = generate_text(model, dataset, \"A\", 20, 1.0)\n",
        "    print(f\"Simple generation result: '{simple_text}'\")\n",
        "except Exception as e:\n",
        "    print(f\"Simple generation failed: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-1cjPE4hsy2",
        "outputId": "6ff71aa1-5383-4bd2-fd8a-ad34ed47b70d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== DEBUGGING MODEL AND DATASET ===\n",
            "Dataset length: 429280\n",
            "Vocabulary size: 109\n",
            "First 10 vocab chars: [' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')']\n",
            "Sequence length: 50\n",
            "Sample input shape: torch.Size([50])\n",
            "Sample target shape: torch.Size([50])\n",
            "Sample input: tensor([ 6,  3, 17, 19, 27,  0, 33, 76, 75, 81])\n",
            "Sample target: tensor([ 3, 17, 19, 27,  0, 33, 76, 75, 81, 66])\n",
            "Model device: cuda:0\n",
            "Model parameters: 963,565\n",
            "Forward pass successful!\n",
            "Output shape: torch.Size([1, 50, 109])\n",
            "Hidden state type: <class 'tuple'>\n",
            "\n",
            "=== SIMPLE GENERATION TEST ===\n",
            "Using device: cuda:0\n",
            "Seed text: 'A'\n",
            "Starting generation with seed length: 1\n",
            "Simple generation result: 'A allocated to captur'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3369287689.py:20: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This is what Block 9 does:\n",
        "sample_text = generate_text(model, dataset, seed_text=\"The \", length=300, temperature=0.7)\n",
        "print(sample_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kWL98hmkbXK",
        "outputId": "9e208d6e-1259-43e1-970d-e6f359c036ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda:0\n",
            "Seed text: 'The '\n",
            "Starting generation with seed length: 4\n",
            "Generated 50/300 characters\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3369287689.py:20: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated 100/300 characters\n",
            "Generated 150/300 characters\n",
            "Generated 200/300 characters\n",
            "Generated 250/300 characters\n",
            "Generated 300/300 characters\n",
            "The kill command to allow the only in order to note that places them the grep command prompt in the first file has been reparated by the configuration files, you can use a new user with the file /etc/resolv.conf file to scan on a process and learning and returns a job to the system is anonymity. The bas\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Next week, you can do this:\n",
        "loaded_dataset, loaded_model = load_dataset_and_model(\n",
        "    \"/content/book_dataset.pkl\",           # Your saved dataset\n",
        "    \"/content/final_text_generator.pth\"    # Your trained model\n",
        ")\n",
        "\n",
        "# Generate more text immediately:\n",
        "new_text = generate_text(loaded_model, loaded_dataset, \"In conclusion\", 500)\n",
        "print(new_text)\n",
        "\n",
        "# Or continue training for better results:\n",
        "# train_model(loaded_model, new_dataloader, num_epochs=5)  # Additional training"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDW3HKjjktq-",
        "outputId": "80e7e39e-377c-4ede-fd32-149daa3854f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset from: /content/book_dataset.pkl\n",
            "Dataset loaded successfully!\n",
            "Dataset keys: ['vocab', 'char_to_idx', 'idx_to_char', 'text', 'seq_length']\n",
            "Loaded dataset - Text length: 429330, Vocab size: 109\n",
            "Loading model on device: cuda\n",
            "Model loaded successfully!\n",
            "Using device: cuda:0\n",
            "Seed text: 'In conclusion'\n",
            "Starting generation with seed length: 13\n",
            "Generated 50/500 characters\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3369287689.py:20: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated 100/500 characters\n",
            "Generated 150/500 characters\n",
            "Generated 200/500 characters\n",
            "Generated 250/500 characters\n",
            "Generated 300/500 characters\n",
            "Generated 350/500 characters\n",
            "Generated 400/500 characters\n",
            "Generated 450/500 characters\n",
            "Generated 500/500 characters\n",
            "In conclusion should be used for the SSH service with configuration for PostgreSQL password to transmit to every directory to string into. And other options contains log files in the CPU. Tand and Database&#13; Compressing with gzip&#13; Here, we can supply the permission enter the following:&#13; kali &gt;cat &gt; hackingskillsEveryone with at by previous scripts, traffic to your PATH variable, so you can delital configuration file, you need to change passwords and then explore the security vulnerability, s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Try different seeds:\n",
        "generate_text(model, dataset, \"Chapter 1\", 200)\n",
        "generate_text(model, dataset, \"Introduction\", 200)\n",
        "generate_text(model, dataset, \"In this book\", 200)\n",
        "\n",
        "# Try different temperatures:\n",
        "generate_text(model, dataset, \"The\", 200, temperature=0.5)  # More focused\n",
        "generate_text(model, dataset, \"The\", 200, temperature=1.2)  # More creative"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "id": "EVR0SkjglGNt",
        "outputId": "ea64a825-e0b2-4a69-a603-25a36d981b59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda:0\n",
            "Seed text: 'Chapter 1'\n",
            "Starting generation with seed length: 9\n",
            "Generated 50/200 characters\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3369287689.py:20: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated 100/200 characters\n",
            "Generated 150/200 characters\n",
            "Generated 200/200 characters\n",
            "Using device: cuda:0\n",
            "Seed text: 'Introduction'\n",
            "Starting generation with seed length: 12\n",
            "Generated 50/200 characters\n",
            "Generated 100/200 characters\n",
            "Generated 150/200 characters\n",
            "Generated 200/200 characters\n",
            "Using device: cuda:0\n",
            "Seed text: 'In this book'\n",
            "Starting generation with seed length: 12\n",
            "Generated 50/200 characters\n",
            "Generated 100/200 characters\n",
            "Generated 150/200 characters\n",
            "Generated 200/200 characters\n",
            "Using device: cuda:0\n",
            "Seed text: 'The'\n",
            "Starting generation with seed length: 3\n",
            "Generated 50/200 characters\n",
            "Generated 100/200 characters\n",
            "Generated 150/200 characters\n",
            "Generated 200/200 characters\n",
            "Using device: cuda:0\n",
            "Seed text: 'The'\n",
            "Starting generation with seed length: 3\n",
            "Generated 50/200 characters\n",
            "Generated 100/200 characters\n",
            "Generated 150/200 characters\n",
            "Generated 200/200 characters\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Then HackersAriseForiate with a best programmers with git clone, and so account, like Apache is constant. When you are the root user by entering a shortcut use if you have IP addresses associated with th'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  -DATAloader issue"
      ],
      "metadata": {
        "id": "GoADjWdUGng-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install ebooklib nltk tqdm\n",
        "\n",
        "# Import all necessary libraries\n",
        "import os\n",
        "import re\n",
        "import pickle\n",
        "import string\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import torch.nn.functional as F\n",
        "from ebooklib import epub\n",
        "import ebooklib\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59ccb9dc-28ee-40df-f742-035189896431",
        "id": "7aaCMGJx97df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ebooklib in /usr/local/lib/python3.11/dist-packages (0.19)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from ebooklib) (5.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from ebooklib) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Upload EPUB file\n",
        "print(\"Please upload your EPUB file:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Get the uploaded filename\n",
        "epub_filename = list(uploaded.keys())[0]\n",
        "epub_path = f\"/content/{epub_filename}\"\n",
        "print(f\"Uploaded file: {epub_filename}\")\n",
        "\n",
        "# Set up NLTK data\n",
        "nltk.download('punkt', quiet=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "56923203-1371-4fa3-b001-042e87308ae0",
        "id": "OoqQwshl-F6r"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please upload your EPUB file:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e4fa202b-a304-42f1-885e-8fab0e1329a5\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e4fa202b-a304-42f1-885e-8fab0e1329a5\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving linux.epub to linux (3).epub\n",
            "Uploaded file: linux (3).epub\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_epub(epub_path):\n",
        "    \"\"\"Extract clean text from EPUB file\"\"\"\n",
        "    try:\n",
        "        book = epub.read_epub(epub_path)\n",
        "        text = \"\"\n",
        "        for item in book.get_items():\n",
        "            if item.get_type() == ebooklib.ITEM_DOCUMENT:\n",
        "                try:\n",
        "                    raw_text = item.get_content().decode('utf-8')\n",
        "                    # Remove HTML tags but preserve text structure\n",
        "                    clean_text = re.sub('<[^<]+?>', '', raw_text)\n",
        "                    text += clean_text + \"\\n\"\n",
        "                except Exception as e:\n",
        "                    print(f\"Warning: Could not decode item: {e}\")\n",
        "                    continue\n",
        "        return text.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading EPUB: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Light preprocessing to preserve character-level information\"\"\"\n",
        "    # Normalize whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    # Remove excessive newlines but keep some structure\n",
        "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
        "    return text.strip()\n",
        "\n",
        "# Extract and preprocess text\n",
        "print(\"Extracting text from EPUB...\")\n",
        "raw_text = extract_text_from_epub(epub_path)\n",
        "cleaned_text = preprocess_text(raw_text)\n",
        "\n",
        "print(f\"Text extracted: {len(cleaned_text)} characters\")\n",
        "print(\"Sample text:\")\n",
        "print(cleaned_text[:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25655876-7518-4566-ef9f-e4189bd817c7",
        "id": "0NSjibzXEYy4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting text from EPUB...\n",
            "Text extracted: 429330 characters\n",
            "Sample text:\n",
            "&#13; Contents in Detail&#13; &#13; Cover Page&#13; Title Page&#13; Copyright Page&#13; Dedication&#13; About the Author&#13; About the Technical Reviewer&#13; BRIEF CONTENTS&#13; CONTENTS IN DETAIL&#13; ACKNOWLEDGMENTS&#13; INTRODUCTION&#13; &#13; What’s in This Book&#13; What Is Ethical Hacking?&#13; Why Hackers Use Linux&#13; Downloading Kali Linux&#13; Virtual Machines&#13; Setting Up Kali&#13; &#13; 1 GETTING STARTED WITH THE BASICS&#13; &#13; Introductory Terms and Concepts&#13; A Tour of \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NEW IMPROVED APPROACH - Token-level modeling\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "class ImprovedBookDataset(Dataset):\n",
        "    def __init__(self, text, seq_length, min_freq=2):\n",
        "        self.seq_length = seq_length\n",
        "\n",
        "        # Better tokenization\n",
        "        self.tokens = self.tokenize_text(text)\n",
        "\n",
        "        # Build vocabulary with minimum frequency filtering\n",
        "        token_counts = Counter(self.tokens)\n",
        "        self.vocab = ['<PAD>', '<UNK>'] + [token for token, count in token_counts.items() if count >= min_freq]\n",
        "\n",
        "        self.token_to_idx = {token: idx for idx, token in enumerate(self.vocab)}\n",
        "        self.idx_to_token = {idx: token for idx, token in enumerate(self.vocab)}\n",
        "\n",
        "        print(f\"Vocabulary size: {len(self.vocab)}\")\n",
        "        print(f\"Total tokens: {len(self.tokens)}\")\n",
        "        print(f\"Unique tokens (before filtering): {len(token_counts)}\")\n",
        "\n",
        "    def tokenize_text(self, text):\n",
        "        # Better tokenization - preserve words, numbers, punctuation\n",
        "        # Split on whitespace and preserve punctuation\n",
        "        tokens = re.findall(r'\\b\\w+\\b|[^\\w\\s]', text)\n",
        "        return [token.lower() for token in tokens if token.strip()]\n",
        "\n",
        "    def __len__(self):\n",
        "        return max(0, len(self.tokens) - self.seq_length)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get sequence of tokens\n",
        "        token_sequence = self.tokens[idx:idx + self.seq_length + 1]\n",
        "\n",
        "        # Pad if sequence is too short\n",
        "        if len(token_sequence) < self.seq_length + 1:\n",
        "            token_sequence.extend(['<PAD>'] * (self.seq_length + 1 - len(token_sequence)))\n",
        "\n",
        "        # Convert to indices\n",
        "        indices = [self.token_to_idx.get(token, self.token_to_idx['<UNK>']) for token in token_sequence]\n",
        "\n",
        "        return torch.tensor(indices[:-1], dtype=torch.long), torch.tensor(indices[1:], dtype=torch.long)\n",
        "\n",
        "# Create improved dataset\n",
        "print(\"Creating improved token-level dataset...\")\n",
        "improved_dataset = ImprovedBookDataset(cleaned_text, seq_length=100, min_freq=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNHFATSNp0D3",
        "outputId": "0e02f840-89c5-4b3c-c14d-fad6c47304b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating improved token-level dataset...\n",
            "Vocabulary size: 3248\n",
            "Total tokens: 105649\n",
            "Unique tokens (before filtering): 5377\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to save the improved token-level dataset\n",
        "def save_improved_dataset(dataset, filepath=\"/content/improved_book_dataset.pkl\"):\n",
        "    \"\"\"Save the improved token-level dataset\"\"\"\n",
        "    try:\n",
        "        dataset_data = {\n",
        "            'tokens': dataset.tokens,\n",
        "            'vocab': dataset.vocab,\n",
        "            'token_to_idx': dataset.token_to_idx,\n",
        "            'idx_to_token': dataset.idx_to_token,\n",
        "            'seq_length': dataset.seq_length\n",
        "        }\n",
        "\n",
        "        with open(filepath, \"wb\") as f:\n",
        "            pickle.dump(dataset_data, f)\n",
        "\n",
        "        print(f\"Improved dataset saved successfully to {filepath}\")\n",
        "        print(f\"Dataset size: {len(dataset.tokens)} tokens\")\n",
        "        print(f\"Vocabulary size: {len(dataset.vocab)} tokens\")\n",
        "\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving dataset: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return False\n",
        "\n",
        "# Function to load the improved token-level dataset\n",
        "def load_improved_dataset(filepath=\"/content/improved_book_dataset.pkl\"):\n",
        "    \"\"\"Load the improved token-level dataset\"\"\"\n",
        "    try:\n",
        "        with open(filepath, \"rb\") as f:\n",
        "            dataset_data = pickle.load(f)\n",
        "\n",
        "        print(f\"Improved dataset loaded from {filepath}\")\n",
        "\n",
        "        # Recreate dataset object\n",
        "        class LoadedImprovedDataset(Dataset):\n",
        "            def __init__(self, data):\n",
        "                self.tokens = data['tokens']\n",
        "                self.vocab = data['vocab']\n",
        "                self.token_to_idx = data['token_to_idx']\n",
        "                self.idx_to_token = data['idx_to_token']\n",
        "                self.seq_length = data['seq_length']\n",
        "\n",
        "                print(f\"Loaded dataset - Tokens: {len(self.tokens)}, Vocab: {len(self.vocab)}\")\n",
        "\n",
        "            def tokenize_text(self, text):\n",
        "                # Reuse the same tokenization logic\n",
        "                tokens = re.findall(r'\\b\\w+\\b|[^\\w\\s]', text)\n",
        "                return [token.lower() for token in tokens if token.strip()]\n",
        "\n",
        "            def __len__(self):\n",
        "                return max(0, len(self.tokens) - self.seq_length)\n",
        "\n",
        "            def __getitem__(self, idx):\n",
        "                token_sequence = self.tokens[idx:idx + self.seq_length + 1]\n",
        "\n",
        "                if len(token_sequence) < self.seq_length + 1:\n",
        "                    token_sequence.extend(['<PAD>'] * (self.seq_length + 1 - len(token_sequence)))\n",
        "\n",
        "                indices = [self.token_to_idx.get(token, self.token_to_idx['<UNK>']) for token in token_sequence]\n",
        "\n",
        "                return torch.tensor(indices[:-1], dtype=torch.long), torch.tensor(indices[1:], dtype=torch.long)\n",
        "\n",
        "        loaded_dataset = LoadedImprovedDataset(dataset_data)\n",
        "        return loaded_dataset\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading dataset: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "# Save the improved dataset\n",
        "print(\"Saving improved dataset...\")\n",
        "save_success = save_improved_dataset(improved_dataset, \"/content/improved_book_dataset.pkl\")\n",
        "\n",
        "if save_success:\n",
        "    print(\"Dataset saved successfully!\")\n",
        "\n",
        "    # Test loading the dataset\n",
        "    print(\"\\nTesting dataset loading...\")\n",
        "    loaded_improved_dataset = load_improved_dataset(\"/content/improved_book_dataset.pkl\")\n",
        "\n",
        "    if loaded_improved_dataset:\n",
        "        print(\"Dataset loaded successfully!\")\n",
        "        print(f\"Loaded dataset info:\")\n",
        "        print(f\"  - Tokens: {len(loaded_improved_dataset.tokens)}\")\n",
        "        print(f\"  - Vocabulary: {len(loaded_improved_dataset.vocab)}\")\n",
        "        print(f\"  - Sample vocab: {loaded_improved_dataset.vocab[:10]}\")\n",
        "\n",
        "        # Test a sample from loaded dataset\n",
        "        if len(loaded_improved_dataset) > 0:\n",
        "            sample_input, sample_target = loaded_improved_dataset[0]\n",
        "            print(f\"  - Sample input shape: {sample_input.shape}\")\n",
        "            print(f\"  - Sample target shape: {sample_target.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7fZd_xqEs3J",
        "outputId": "39dc2174-09fc-4e70-f079-5c7378bd4189"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving improved dataset...\n",
            "Improved dataset saved successfully to /content/improved_book_dataset.pkl\n",
            "Dataset size: 105649 tokens\n",
            "Vocabulary size: 3248 tokens\n",
            "Dataset saved successfully!\n",
            "\n",
            "Testing dataset loading...\n",
            "Improved dataset loaded from /content/improved_book_dataset.pkl\n",
            "Loaded dataset - Tokens: 105649, Vocab: 3248\n",
            "Dataset loaded successfully!\n",
            "Loaded dataset info:\n",
            "  - Tokens: 105649\n",
            "  - Vocabulary: 3248\n",
            "  - Sample vocab: ['<PAD>', '<UNK>', '&', '#', '13', ';', 'contents', 'in', 'detail', 'cover']\n",
            "  - Sample input shape: torch.Size([100])\n",
            "  - Sample target shape: torch.Size([100])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EnhancedTextGenerator(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=256, hidden_dim=512, num_layers=3, dropout=0.3):\n",
        "        super(EnhancedTextGenerator, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        # Larger embedding dimension\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "\n",
        "        # Multi-layer LSTM with more capacity\n",
        "        self.lstm = nn.LSTM(\n",
        "            embedding_dim,\n",
        "            hidden_dim,\n",
        "            num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0,\n",
        "            bidirectional=False\n",
        "        )\n",
        "\n",
        "        # Additional layers for better learning\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
        "        self.fc1 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
        "        self.fc2 = nn.Linear(hidden_dim // 2, vocab_size)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        # Embedding\n",
        "        embedded = self.embedding(x)\n",
        "\n",
        "        # LSTM\n",
        "        lstm_out, hidden = self.lstm(embedded, hidden)\n",
        "\n",
        "        # Normalization and dropout\n",
        "        lstm_out = self.layer_norm(lstm_out)\n",
        "        lstm_out = self.dropout(lstm_out)\n",
        "\n",
        "        # Additional layers\n",
        "        out = self.relu(self.fc1(lstm_out))\n",
        "        out = self.dropout(out)\n",
        "        output = self.fc2(out)\n",
        "\n",
        "        return output, hidden\n",
        "\n",
        "# Initialize enhanced model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "enhanced_model = EnhancedTextGenerator(\n",
        "    vocab_size=len(improved_dataset.vocab),\n",
        "    embedding_dim=256,\n",
        "    hidden_dim=512,\n",
        "    num_layers=3,\n",
        "    dropout=0.3\n",
        ").to(device)\n",
        "\n",
        "print(f\"Enhanced model parameters: {sum(p.numel() for p in enhanced_model.parameters()):,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHq1nFeEqGxS",
        "outputId": "264d67ca-7671-49ad-ed16-06b1137fe6aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enhanced model parameters: 7,578,032\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Debug information before training\n",
        "print(\"=== DEBUG INFORMATION ===\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name()}\")\n",
        "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
        "\n",
        "print(f\"Model device: {next(enhanced_model.parameters()).device}\")\n",
        "print(f\"Dataset size: {len(improved_dataset)}\")\n",
        "print(f\"Vocabulary size: {len(improved_dataset.vocab)}\")\n",
        "\n",
        "# Test dataset sample\n",
        "if len(improved_dataset) > 0:\n",
        "    sample_input, sample_target = improved_dataset[0]\n",
        "    print(f\"Sample input shape: {sample_input.shape}\")\n",
        "    print(f\"Sample target shape: {sample_target.shape}\")\n",
        "    print(f\"Sample input[:10]: {sample_input[:10]}\")\n",
        "    print(f\"Sample target[:10]: {sample_target[:10]}\")\n",
        "\n",
        "# Test model forward pass\n",
        "try:\n",
        "    test_input = sample_input.unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        test_output, _ = enhanced_model(test_input)\n",
        "        print(f\"Test output shape: {test_output.shape}\")\n",
        "        print(\"✅ Model forward pass successful!\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Model forward pass failed: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9Mgq3Jf5WvO",
        "outputId": "281fbe4c-8cd2-41ac-fe9b-590bd4a16296"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== DEBUG INFORMATION ===\n",
            "CUDA available: False\n",
            "Model device: cpu\n",
            "Dataset size: 105549\n",
            "Vocabulary size: 3248\n",
            "Sample input shape: torch.Size([100])\n",
            "Sample target shape: torch.Size([100])\n",
            "Sample input[:10]: tensor([2, 3, 4, 5, 6, 7, 8, 2, 3, 4])\n",
            "Sample target[:10]: tensor([3, 4, 5, 6, 7, 8, 2, 3, 4, 5])\n",
            "Test output shape: torch.Size([1, 100, 3248])\n",
            "✅ Model forward pass successful!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FIXED Enhanced Text Generator Model\n",
        "class FixedEnhancedTextGenerator(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=256, hidden_dim=512, num_layers=2, dropout=0.3):\n",
        "        super(FixedEnhancedTextGenerator, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "\n",
        "        # LSTM layer with proper initialization\n",
        "        self.lstm = nn.LSTM(\n",
        "            embedding_dim,\n",
        "            hidden_dim,\n",
        "            num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0,\n",
        "            bidirectional=False\n",
        "        )\n",
        "\n",
        "        # Output layers\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def init_hidden(self, batch_size, device):\n",
        "        \"\"\"Initialize hidden state properly\"\"\"\n",
        "        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
        "        cell = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
        "        return (hidden, cell)\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        # Get device from input\n",
        "        device = x.device\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Embedding\n",
        "        embedded = self.embedding(x)  # (batch, seq_len) -> (batch, seq_len, embedding_dim)\n",
        "\n",
        "        # Initialize hidden state if not provided\n",
        "        if hidden is None:\n",
        "            hidden = self.init_hidden(batch_size, device)\n",
        "\n",
        "        # LSTM forward pass\n",
        "        lstm_out, hidden = self.lstm(embedded, hidden)\n",
        "\n",
        "        # Dropout\n",
        "        lstm_out = self.dropout(lstm_out)\n",
        "\n",
        "        # Output projection\n",
        "        output = self.fc(lstm_out)\n",
        "\n",
        "        return output, hidden\n",
        "\n",
        "# Create the fixed model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "fixed_enhanced_model = FixedEnhancedTextGenerator(\n",
        "    vocab_size=len(improved_dataset.vocab),\n",
        "    embedding_dim=256,\n",
        "    hidden_dim=512,\n",
        "    num_layers=2,  # Reduced from 3 to 2 for stability\n",
        "    dropout=0.3\n",
        ").to(device)\n",
        "\n",
        "print(f\"Fixed model parameters: {sum(p.numel() for p in fixed_enhanced_model.parameters()):,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OvhcrxbEGad",
        "outputId": "c1ec6cc6-8ca2-4d65-8a62-51e5dd864c76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Fixed model parameters: 6,175,920\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the enhanced dataloader\n",
        "print(\"Creating enhanced dataloader...\")\n",
        "enhanced_dataloader = DataLoader(\n",
        "    improved_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    num_workers=2 if torch.cuda.is_available() else 0,  # 0 for CPU to avoid issues\n",
        "    pin_memory=torch.cuda.is_available(),\n",
        "    persistent_workers=True if torch.cuda.is_available() else False\n",
        ")\n",
        "\n",
        "print(f\"Dataloader created successfully!\")\n",
        "print(f\"Number of batches: {len(enhanced_dataloader)}\")\n",
        "print(f\"Batch size: {enhanced_dataloader.batch_size}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJ7rMPutFrIj",
        "outputId": "84f4ad8b-be65-4435-e524-c6ae5f1e77ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating enhanced dataloader...\n",
            "Dataloader created successfully!\n",
            "Number of batches: 3299\n",
            "Batch size: 32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define loss function and other components\n",
        "enhanced_criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding tokens\n",
        "\n",
        "print(\"Enhanced components initialized!\")\n",
        "print(f\"Vocabulary size: {len(improved_dataset.vocab)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyK76QJ1FvS7",
        "outputId": "eae4fe93-aa1b-48cb-e351-fae40aff9c12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enhanced components initialized!\n",
            "Vocabulary size: 3248\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the fixed model\n",
        "print(\"=== Testing Fixed Model ===\")\n",
        "\n",
        "# Get a sample batch\n",
        "try:\n",
        "    for batch_idx, (inputs, targets) in enumerate(enhanced_dataloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        print(f\"Input shape: {inputs.shape}\")\n",
        "        print(f\"Target shape: {targets.shape}\")\n",
        "\n",
        "        # Test forward pass\n",
        "        with torch.no_grad():\n",
        "            outputs, hidden = fixed_enhanced_model(inputs)\n",
        "            print(f\"Output shape: {outputs.shape}\")\n",
        "            print(f\"Hidden state type: {type(hidden)}\")\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = enhanced_criterion(outputs.reshape(-1, outputs.size(-1)), targets.reshape(-1))\n",
        "            print(f\"Test loss: {loss.item():.4f}\")\n",
        "        break\n",
        "\n",
        "    print(\"✅ Fixed model test successful!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Fixed model test failed: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHEvYu7ZFCW9",
        "outputId": "5e02d760-0248-4fc8-8e8d-e0c6f9078163"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Testing Fixed Model ===\n",
            "Input shape: torch.Size([32, 100])\n",
            "Target shape: torch.Size([32, 100])\n",
            "Output shape: torch.Size([32, 100, 3248])\n",
            "Hidden state type: <class 'tuple'>\n",
            "Test loss: 8.0829\n",
            "✅ Fixed model test successful!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FIXED Training Function\n",
        "from torch.amp import autocast\n",
        "\n",
        "def fixed_enhanced_train_model(model, dataloader, num_epochs=10):\n",
        "    \"\"\"Fixed training function with proper error handling\"\"\"\n",
        "\n",
        "    # Optimizer and scheduler\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "    scaler = torch.cuda.amp.GradScaler() if torch.cuda.is_available() else None\n",
        "\n",
        "    train_losses = []\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    device = next(model.parameters()).device\n",
        "    use_amp = torch.cuda.is_available() and scaler is not None\n",
        "\n",
        "    print(f\"Training on device: {device}\")\n",
        "    print(f\"Using AMP: {use_amp}\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        num_batches = 0\n",
        "\n",
        "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        for batch_idx, (inputs, targets) in enumerate(progress_bar):\n",
        "            try:\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Forward pass\n",
        "                if use_amp:\n",
        "                    with autocast('cuda'):\n",
        "                        outputs, _ = model(inputs)\n",
        "                        outputs = outputs.reshape(-1, outputs.size(-1))\n",
        "                        targets = targets.reshape(-1)\n",
        "                        loss = criterion(outputs, targets)\n",
        "\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.unscale_(optimizer)\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "                    scaler.step(optimizer)\n",
        "                    scaler.update()\n",
        "                else:\n",
        "                    outputs, _ = model(inputs)\n",
        "                    outputs = outputs.reshape(-1, outputs.size(-1))\n",
        "                    targets = targets.reshape(-1)\n",
        "                    loss = criterion(outputs, targets)\n",
        "\n",
        "                    loss.backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "                    optimizer.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                num_batches += 1\n",
        "\n",
        "                # Update progress bar\n",
        "                progress_bar.set_postfix({\n",
        "                    'loss': f'{loss.item():.4f}',\n",
        "                    'avg_loss': f'{total_loss/num_batches:.4f}'\n",
        "                })\n",
        "\n",
        "                # Clear cache periodically\n",
        "                if batch_idx % 200 == 0 and torch.cuda.is_available():\n",
        "                    torch.cuda.empty_cache()\n",
        "\n",
        "            except RuntimeError as e:\n",
        "                if \"out of memory\" in str(e):\n",
        "                    print(f\"⚠️  Out of memory at batch {batch_idx}, skipping...\")\n",
        "                    if torch.cuda.is_available():\n",
        "                        torch.cuda.empty_cache()\n",
        "                    continue\n",
        "                else:\n",
        "                    raise e\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️  Error in batch {batch_idx}: {e}\")\n",
        "                continue\n",
        "\n",
        "        # Calculate epoch statistics\n",
        "        if num_batches > 0:\n",
        "            avg_loss = total_loss / num_batches\n",
        "            train_losses.append(avg_loss)\n",
        "\n",
        "            # Update learning rate\n",
        "            scheduler.step(avg_loss)\n",
        "\n",
        "            print(f\"Epoch {epoch+1}/{num_epochs} - Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "            # Save best model\n",
        "            if avg_loss < best_loss:\n",
        "                best_loss = avg_loss\n",
        "                torch.save({\n",
        "                    'epoch': epoch,\n",
        "                    'model_state_dict': model.state_dict(),\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\n",
        "                    'loss': avg_loss,\n",
        "                    'vocab': improved_dataset.vocab,\n",
        "                    'token_to_idx': improved_dataset.token_to_idx,\n",
        "                    'idx_to_token': improved_dataset.idx_to_token\n",
        "                }, \"/content/best_fixed_model.pth\")\n",
        "                print(f\"⭐ New best model saved! Loss: {avg_loss:.4f}\")\n",
        "        else:\n",
        "            print(f\"Epoch {epoch+1}/{num_epochs} - No batches processed\")\n",
        "\n",
        "    return train_losses\n",
        "\n",
        "# Test single batch before full training\n",
        "print(\"\\n=== Pre-training Test ===\")\n",
        "try:\n",
        "    for inputs, targets in enhanced_dataloader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        print(f\"Test batch - Input: {inputs.shape}, Target: {targets.shape}\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs, _ = fixed_enhanced_model(inputs)\n",
        "            print(f\"Model output: {outputs.shape}\")\n",
        "        break\n",
        "    print(\"✅ Pre-training test passed!\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Pre-training test failed: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "# Start training\n",
        "print(\"\\n🚀 Starting fixed enhanced training...\")\n",
        "try:\n",
        "    fixed_losses = fixed_enhanced_train_model(fixed_enhanced_model, enhanced_dataloader, num_epochs=10)\n",
        "    print(\"🎉 Training completed successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Training failed: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "Bda5AQVMFMa4",
        "outputId": "0ed65804-0827-4026-a468-b0a44683d184"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Pre-training Test ===\n",
            "Test batch - Input: torch.Size([32, 100]), Target: torch.Size([32, 100])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2167298317.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfixed_enhanced_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Model output: {outputs.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1309488391.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, hidden)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m# Dropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mlstm_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# Output projection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/dropout.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1423\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"dropout probability has to be between 0 and 1, but got {p}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m     return (\n\u001b[0;32m-> 1425\u001b[0;31m         \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1426\u001b[0m     )\n\u001b[1;32m   1427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test text generation with the trained model\n",
        "def test_generation(model, dataset, seed_text=\"The\", max_length=50):\n",
        "    \"\"\"Simple generation test\"\"\"\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # Tokenize seed\n",
        "    seed_tokens = dataset.tokenize_text(seed_text)\n",
        "    seed_indices = [dataset.token_to_idx.get(token, dataset.token_to_idx['<UNK>']) for token in seed_tokens]\n",
        "\n",
        "    input_seq = torch.tensor([seed_indices], dtype=torch.long).to(device)\n",
        "    generated_tokens = seed_tokens.copy()\n",
        "\n",
        "    hidden = None\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_length):\n",
        "            output, hidden = model(input_seq, hidden)\n",
        "            # Get last token prediction\n",
        "            logits = output[0, -1]\n",
        "            # Get most likely token\n",
        "            next_token_idx = torch.argmax(logits).item()\n",
        "\n",
        "            if next_token_idx < len(dataset.idx_to_token):\n",
        "                next_token = dataset.idx_to_token[next_token_idx]\n",
        "                if next_token != '<PAD>':\n",
        "                    generated_tokens.append(next_token)\n",
        "\n",
        "            # Update input\n",
        "            new_input = torch.tensor([[next_token_idx]], device=device)\n",
        "            input_seq = torch.cat([input_seq, new_input], dim=1)\n",
        "            if input_seq.size(1) > 100:\n",
        "                input_seq = input_seq[:, -100:]\n",
        "\n",
        "    return ' '.join(generated_tokens)\n",
        "\n",
        "# Test generation\n",
        "print(\"\\n=== Testing Text Generation ===\")\n",
        "try:\n",
        "    test_text = test_generation(fixed_enhanced_model, improved_dataset, \"The\", 30)\n",
        "    print(\"Generated text:\")\n",
        "    print(test_text)\n",
        "except Exception as e:\n",
        "    print(f\"Generation test failed: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdzjbg5VF69y",
        "outputId": "f6cddfb0-0198-442b-b6e5-e6327031127e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Testing Text Generation ===\n",
            "Generated text:\n",
            "the # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model and dataset info\n",
        "print(\"\\n=== Saving Model and Dataset ===\")\n",
        "try:\n",
        "    # Save model\n",
        "    torch.save({\n",
        "        'model_state_dict': fixed_enhanced_model.state_dict(),\n",
        "        'vocab': improved_dataset.vocab,\n",
        "        'token_to_idx': improved_dataset.token_to_idx,\n",
        "        'idx_to_token': improved_dataset.idx_to_token,\n",
        "        'model_config': {\n",
        "            'vocab_size': len(improved_dataset.vocab),\n",
        "            'embedding_dim': 256,\n",
        "            'hidden_dim': 512,\n",
        "            'num_layers': 2\n",
        "        }\n",
        "    }, \"/content/final_trained_model.pth\")\n",
        "\n",
        "    # Save dataset info\n",
        "    import pickle\n",
        "    with open(\"/content/dataset_info.pkl\", \"wb\") as f:\n",
        "        pickle.dump({\n",
        "            'vocab': improved_dataset.vocab,\n",
        "            'token_to_idx': improved_dataset.token_to_idx,\n",
        "            'idx_to_token': improved_dataset.idx_to_token,\n",
        "            'text_sample': cleaned_text[:1000]  # Save a sample for reference\n",
        "        }, f)\n",
        "\n",
        "    print(\"✅ Model and dataset saved successfully!\")\n",
        "    print(\"Files saved:\")\n",
        "    print(\"  - /content/final_trained_model.pth\")\n",
        "    print(\"  - /content/dataset_info.pkl\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Failed to save: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ],
      "metadata": {
        "id": "Tin7FI_oGE8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# resoucess cheack-up"
      ],
      "metadata": {
        "id": "QymuqCjuJw-B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import psutil\n",
        "import torch\n",
        "import os\n",
        "import time\n",
        "\n",
        "def check_gpu():\n",
        "    \"\"\"Check if GPU is available and accessible.\"\"\"\n",
        "    try:\n",
        "        # Check via nvidia-smi\n",
        "        result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "        if result.returncode != 0:\n",
        "            return {\"available\": False, \"message\": \"GPU not available (nvidia-smi failed)\", \"details\": result.stderr}\n",
        "\n",
        "        # Check PyTorch CUDA\n",
        "        if torch.cuda.is_available():\n",
        "            gpu_name = torch.cuda.get_device_name(0)\n",
        "            return {\"available\": True, \"message\": \"GPU is available\", \"name\": gpu_name}\n",
        "        else:\n",
        "            return {\"available\": False, \"message\": \"CUDA not available in PyTorch\", \"details\": \"nvidia-smi works but PyTorch can't access GPU\"}\n",
        "    except Exception as e:\n",
        "        return {\"available\": False, \"message\": \"Error checking GPU\", \"error\": str(e)}\n",
        "\n",
        "def check_ram():\n",
        "    \"\"\"Check available RAM.\"\"\"\n",
        "    ram_gb = psutil.virtual_memory().available / (1024**3)\n",
        "    return {\"available_gb\": round(ram_gb, 2)}\n",
        "\n",
        "def check_disk():\n",
        "    \"\"\"Check available disk space.\"\"\"\n",
        "    disk = psutil.disk_usage(\"/\")\n",
        "    return {\"available_gb\": round(disk.free / (1024**3), 2)}\n",
        "\n",
        "def check_execution_speed():\n",
        "    \"\"\"Run a small compute task to estimate performance (detect throttling).\"\"\"\n",
        "    try:\n",
        "        start = time.time()\n",
        "        # Perform a small matrix operation\n",
        "        x = torch.randn(1000, 1000)\n",
        "        y = torch.randn(1000, 1000)\n",
        "        for _ in range(10):\n",
        "            z = torch.mm(x, y)\n",
        "        duration = time.time() - start\n",
        "        return {\"duration_seconds\": round(duration, 3), \"fast\": duration < 2.0}\n",
        "    except Exception as e:\n",
        "        return {\"error\": str(e)}\n",
        "\n",
        "def check_tpu():\n",
        "    \"\"\"Check if TPU is available (Colab Pro+ or specific allocation).\"\"\"\n",
        "    try:\n",
        "        import tensorflow as tf\n",
        "        resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n",
        "        tf.config.experimental_connect_to_cluster(resolver)\n",
        "        tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "        return {\"available\": True, \"message\": \"TPU is available\"}\n",
        "    except Exception as e:\n",
        "        return {\"available\": False, \"message\": \"TPU not available\", \"error\": str(e)}\n",
        "\n",
        "def check_runtime_type():\n",
        "    \"\"\"Check the runtime environment.\"\"\"\n",
        "    try:\n",
        "        # This file exists in Colab\n",
        "        import google.colab\n",
        "        return \"colab\"\n",
        "    except:\n",
        "        return \"not_colab\"\n",
        "\n",
        "def check_usage_limits():\n",
        "    \"\"\"Main function to check overall usability.\"\"\"\n",
        "    print(\"🔍 Checking Google Colab Resource Availability...\\n\")\n",
        "\n",
        "    # 1. Confirm we're in Colab\n",
        "    runtime = check_runtime_type()\n",
        "    if runtime != \"colab\":\n",
        "        print(\"❌ This script is not running in Google Colab.\")\n",
        "        return\n",
        "\n",
        "    # 2. Check GPU\n",
        "    gpu = check_gpu()\n",
        "    print(f\"🎮 GPU: {'✅ Available' if gpu['available'] else '❌ Not Available'}\")\n",
        "    if gpu['available']:\n",
        "        print(f\"   → Model: {gpu['name']}\")\n",
        "    else:\n",
        "        print(f\"   → {gpu['message']}\")\n",
        "\n",
        "    # 3. Check RAM\n",
        "    ram = check_ram()\n",
        "    print(f\"🧠 RAM Available: {ram['available_gb']} GB\")\n",
        "\n",
        "    # 4. Check Disk\n",
        "    disk = check_disk()\n",
        "    print(f\"💾 Disk Available: {disk['available_gb']} GB\")\n",
        "\n",
        "    # 5. Check Execution Speed (heuristic for throttling)\n",
        "    print(\"⏱️ Running speed test (small matrix ops)...\")\n",
        "    speed = check_execution_speed()\n",
        "    if \"error\" in speed:\n",
        "        print(f\"   → Speed test failed: {speed['error']}\")\n",
        "    else:\n",
        "        print(f\"   → Speed test took {speed['duration_seconds']} seconds\")\n",
        "        if speed['fast']:\n",
        "            print(\"   → ✅ Likely normal performance\")\n",
        "        else:\n",
        "            print(\"   → ⚠️  Slow execution — possible resource throttling or limitation\")\n",
        "\n",
        "    # 6. Optional: Check TPU\n",
        "    print(\"⚡ Checking TPU...\")\n",
        "    tpu = check_tpu()\n",
        "    print(f\"   → TPU: {'✅ Available' if tpu['available'] else '❌ Not Available'}\")\n",
        "\n",
        "    # Final Assessment\n",
        "    print(\"\\n📝 Final Assessment:\")\n",
        "    if not gpu['available']:\n",
        "        print(\"⚠️  Limited or restricted runtime: GPU not accessible — likely under cool-down or usage limit.\")\n",
        "    elif speed.get(\"duration_seconds\", 10) > 3.0:\n",
        "        print(\"⚠️  Performance is slow — possible throttling or low-tier allocation.\")\n",
        "    elif ram['available_gb'] < 5:\n",
        "        print(\"⚠️  Low RAM — may not support large models.\")\n",
        "    else:\n",
        "        print(\"✅ This Colab session appears to have good resource availability and is usable.\")\n",
        "\n",
        "    print(\"\\n💡 Tip: If resources are limited, try reconnecting or using Colab Pro.\")\n",
        "\n",
        "# Run the check\n",
        "check_usage_limits()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PeaLjPLLJlvu",
        "outputId": "263d35df-b446-4620-d7ce-1701955252fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Checking Google Colab Resource Availability...\n",
            "\n",
            "🎮 GPU: ❌ Not Available\n",
            "   → Error checking GPU\n",
            "🧠 RAM Available: 11.47 GB\n",
            "💾 Disk Available: 69.4 GB\n",
            "⏱️ Running speed test (small matrix ops)...\n",
            "   → Speed test took 0.705 seconds\n",
            "   → ✅ Likely normal performance\n",
            "⚡ Checking TPU...\n",
            "   → TPU: ❌ Not Available\n",
            "\n",
            "📝 Final Assessment:\n",
            "⚠️  Limited or restricted runtime: GPU not accessible — likely under cool-down or usage limit.\n",
            "\n",
            "💡 Tip: If resources are limited, try reconnecting or using Colab Pro.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import torch\n",
        "import time\n",
        "import requests\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "def check_gpu_usability():\n",
        "    \"\"\"\n",
        "    Check if GPU is usable or likely under cool-down/restriction.\n",
        "    Attempts to infer cool-down state and estimate time until recovery.\n",
        "    \"\"\"\n",
        "    print(\"🔍 Assessing GPU Usability and Cool-Down Status...\\n\")\n",
        "\n",
        "    # 1. Check if nvidia-smi is accessible\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            ['nvidia-smi'],\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.PIPE,\n",
        "            text=True,\n",
        "            timeout=10\n",
        "        )\n",
        "        if result.returncode != 0:\n",
        "            print(\"❌ nvidia-smi failed — GPU likely not allocated.\")\n",
        "            print(f\"   Error: {result.stderr.strip()}\")\n",
        "            _suggest_cool_down()\n",
        "            return\n",
        "        else:\n",
        "            print(\"✅ nvidia-smi succeeded — GPU device detected.\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error running nvidia-smi: {e}\")\n",
        "        _suggest_cool_down()\n",
        "        return\n",
        "\n",
        "    # 2. Check PyTorch CUDA\n",
        "    if not torch.cuda.is_available():\n",
        "        print(\"❌ CUDA is not available in PyTorch — GPU cannot be used.\")\n",
        "        _suggest_cool_down()\n",
        "        return\n",
        "    else:\n",
        "        gpu_name = torch.cuda.get_device_name(0)\n",
        "        print(f\"✅ CUDA is available!\")\n",
        "        print(f\"   → GPU Model: {gpu_name}\")\n",
        "\n",
        "    # 3. Run a small GPU computation test\n",
        "    try:\n",
        "        print(\"🧪 Running GPU compute test...\")\n",
        "        x = torch.randn(1000, 1000).cuda()\n",
        "        y = torch.randn(1000, 1000).cuda()\n",
        "        start = time.time()\n",
        "        for _ in range(10):\n",
        "            z = torch.mm(x, y)\n",
        "        torch.cuda.synchronize()\n",
        "        duration = time.time() - start\n",
        "        print(f\"   → GPU test completed in {duration:.3f} seconds\")\n",
        "        if duration > 5.0:\n",
        "            print(\"⚠️  Slow GPU — possible throttling or low-priority allocation.\")\n",
        "        else:\n",
        "            print(\"✅ GPU is responsive and usable.\")\n",
        "        return  # GPU is fully usable\n",
        "    except Exception as e:\n",
        "        print(f\"❌ GPU computation failed: {e}\")\n",
        "        _suggest_cool_down()\n",
        "        return\n",
        "\n",
        "def _suggest_cool_down():\n",
        "    \"\"\"\n",
        "    Provide educated guess about cool-down status and recovery time.\n",
        "    \"\"\"\n",
        "    print(\"\\n🚨 Likely GPU Unavailable Due To:\")\n",
        "    print(\"   • Usage limit reached\")\n",
        "    print(\"   • Session duration or compute quota exceeded\")\n",
        "    print(\"   • Account-level restrictions (especially on free tier)\")\n",
        "\n",
        "    print(\"\\n📅 Estimated Recovery Time:\")\n",
        "\n",
        "    # Heuristic based on common Colab behavior\n",
        "    print(\"   • ⏳ Free Tier: 6–24 hours (often ~12 hours)\")\n",
        "    print(\"   • 💠 Colab Pro: 2–8 hours (depends on usage)\")\n",
        "    print(\"   • 💎 Pro+: 1–4 hours (shorter cooldowns)\")\n",
        "\n",
        "    print(\"\\n💡 Tips to Recover Faster:\")\n",
        "    print(\"   • Avoid running heavy workloads for > 8–12 hours continuously.\")\n",
        "    print(\"   • Disconnect and close the notebook for several hours.\")\n",
        "    print(\"   • Try accessing Colab from a different browser/incognito after 6+ hours.\")\n",
        "    print(\"   • Consider upgrading to Colab Pro/Pro+ for better access.\")\n",
        "    print(\"   • Use lightweight models or CPU when possible during cooldown.\")\n",
        "\n",
        "    # Optional: Show current time and estimate\n",
        "    now = datetime.now()\n",
        "    est_recovery = now + timedelta(hours=12)\n",
        "    print(f\"\\n📌 Estimated earliest recovery: {est_recovery.strftime('%Y-%m-%d %H:%M')} (local time)\")\n",
        "    print(\"   → This is an estimate — actual time may vary.\")\n",
        "\n",
        "# Run the check\n",
        "check_gpu_usability()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHbp8QbKJ-9Z",
        "outputId": "aed3b902-bca4-42c2-a6cd-011753d65a7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Assessing GPU Usability and Cool-Down Status...\n",
            "\n",
            "❌ Error running nvidia-smi: [Errno 2] No such file or directory: 'nvidia-smi'\n",
            "\n",
            "🚨 Likely GPU Unavailable Due To:\n",
            "   • Usage limit reached\n",
            "   • Session duration or compute quota exceeded\n",
            "   • Account-level restrictions (especially on free tier)\n",
            "\n",
            "📅 Estimated Recovery Time:\n",
            "   • ⏳ Free Tier: 6–24 hours (often ~12 hours)\n",
            "   • 💠 Colab Pro: 2–8 hours (depends on usage)\n",
            "   • 💎 Pro+: 1–4 hours (shorter cooldowns)\n",
            "\n",
            "💡 Tips to Recover Faster:\n",
            "   • Avoid running heavy workloads for > 8–12 hours continuously.\n",
            "   • Disconnect and close the notebook for several hours.\n",
            "   • Try accessing Colab from a different browser/incognito after 6+ hours.\n",
            "   • Consider upgrading to Colab Pro/Pro+ for better access.\n",
            "   • Use lightweight models or CPU when possible during cooldown.\n",
            "\n",
            "📌 Estimated earliest recovery: 2025-08-14 22:02 (local time)\n",
            "   → This is an estimate — actual time may vary.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import psutil\n",
        "import platform\n",
        "import subprocess\n",
        "from datetime import datetime\n",
        "\n",
        "def get_cpu_info():\n",
        "    print(\"🧮 CPU Information\\n\")\n",
        "\n",
        "    # Basic system info\n",
        "    print(f\"System: {platform.system()} {platform.release()}\")\n",
        "    print(f\"Architecture: {platform.machine()} ({platform.architecture()[0]})\")\n",
        "    print(f\"Node (Hostname): {platform.node()}\")\n",
        "    print(f\"Python Version: {platform.python_version()}\")\n",
        "    print()\n",
        "\n",
        "    # CPU model\n",
        "    try:\n",
        "        # Try to get CPU info from /proc/cpuinfo (Linux, including Colab)\n",
        "        if platform.system() == \"Linux\":\n",
        "            result = subprocess.run(\n",
        "                [\"cat\", \"/proc/cpuinfo\"],\n",
        "                stdout=subprocess.PIPE,\n",
        "                text=True\n",
        "            )\n",
        "            cpu_info = result.stdout\n",
        "            for line in cpu_info.split(\"\\n\"):\n",
        "                if \"model name\" in line:\n",
        "                    model_name = line.split(\":\")[1].strip()\n",
        "                    print(f\"CPU Model: {model_name}\")\n",
        "                    break\n",
        "        else:\n",
        "            # Fallback for non-Linux\n",
        "            print(f\"CPU Model: {platform.processor()}\")\n",
        "    except Exception as e:\n",
        "        print(f\"CPU Model: Could not retrieve ({e})\")\n",
        "\n",
        "    # Number of cores\n",
        "    physical_cores = psutil.cpu_count(logical=False)\n",
        "    logical_cores = psutil.cpu_count(logical=True)\n",
        "    print(f\"Physical Cores: {physical_cores}\")\n",
        "    print(f\"Logical Cores (Hyperthreading): {logical_cores}\")\n",
        "\n",
        "    # CPU frequency\n",
        "    cpufreq = psutil.cpu_freq()\n",
        "    if cpufreq:\n",
        "        print(f\"Current Frequency: {cpufreq.current:.2f} MHz\")\n",
        "        print(f\"Max Frequency: {cpufreq.max:.2f} MHz\")\n",
        "        print(f\"Min Frequency: {cpufreq.min:.2f} MHz\")\n",
        "    else:\n",
        "        print(\"CPU Frequency: Not available\")\n",
        "\n",
        "    # CPU usage\n",
        "    cpu_usage = psutil.cpu_percent(percpu=False, interval=1)\n",
        "    print(f\"Overall CPU Usage: {cpu_usage}%\")\n",
        "\n",
        "    # Per-core usage (optional, can be long)\n",
        "    print(\"CPU Usage Per Core:\", end=\" \")\n",
        "    for i, usage in enumerate(psutil.cpu_percent(percpu=True, interval=1)):\n",
        "        print(f\"Core {i}: {usage}%\", end=\" | \")\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # Boot time\n",
        "    boot_time = datetime.fromtimestamp(psutil.boot_time())\n",
        "    print(f\"System Boot Time: {boot_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "# Run the function\n",
        "get_cpu_info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3sMmrO-PKOlw",
        "outputId": "429a1841-15e7-4a46-a57c-9218aa490aaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧮 CPU Information\n",
            "\n",
            "System: Linux 6.1.123+\n",
            "Architecture: x86_64 (64bit)\n",
            "Node (Hostname): bd1ce50d9fcf\n",
            "Python Version: 3.11.13\n",
            "\n",
            "CPU Model: AMD EPYC 7B12\n",
            "Physical Cores: 1\n",
            "Logical Cores (Hyperthreading): 2\n",
            "Current Frequency: 2250.00 MHz\n",
            "Max Frequency: 0.00 MHz\n",
            "Min Frequency: 0.00 MHz\n",
            "Overall CPU Usage: 58.6%\n",
            "CPU Usage Per Core: Core 0: 64.0% | Core 1: 37.8% | \n",
            "\n",
            "System Boot Time: 2025-08-14 09:59:42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!lscpu  # to see CPU info\n",
        "!nvidia-smi  # to check GPU"
      ],
      "metadata": {
        "id": "xX1-fPEvKa_W",
        "outputId": "06a7936a-3e24-42ee-f888-0d7c0ad845eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Architecture:             x86_64\n",
            "  CPU op-mode(s):         32-bit, 64-bit\n",
            "  Address sizes:          46 bits physical, 48 bits virtual\n",
            "  Byte Order:             Little Endian\n",
            "CPU(s):                   2\n",
            "  On-line CPU(s) list:    0,1\n",
            "Vendor ID:                GenuineIntel\n",
            "  Model name:             Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "    CPU family:           6\n",
            "    Model:                85\n",
            "    Thread(s) per core:   2\n",
            "    Core(s) per socket:   1\n",
            "    Socket(s):            1\n",
            "    Stepping:             3\n",
            "    BogoMIPS:             4000.30\n",
            "    Flags:                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge m\n",
            "                          ca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht sysc\n",
            "                          all nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xt\n",
            "                          opology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq\n",
            "                           ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt\n",
            "                           aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dno\n",
            "                          wprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase\n",
            "                           tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm m\n",
            "                          px avx512f avx512dq rdseed adx smap clflushopt clwb av\n",
            "                          x512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsave\n",
            "                          s arat md_clear arch_capabilities\n",
            "Virtualization features:  \n",
            "  Hypervisor vendor:      KVM\n",
            "  Virtualization type:    full\n",
            "Caches (sum of all):      \n",
            "  L1d:                    32 KiB (1 instance)\n",
            "  L1i:                    32 KiB (1 instance)\n",
            "  L2:                     1 MiB (1 instance)\n",
            "  L3:                     38.5 MiB (1 instance)\n",
            "NUMA:                     \n",
            "  NUMA node(s):           1\n",
            "  NUMA node0 CPU(s):      0,1\n",
            "Vulnerabilities:          \n",
            "  Gather data sampling:   Not affected\n",
            "  Itlb multihit:          Not affected\n",
            "  L1tf:                   Mitigation; PTE Inversion\n",
            "  Mds:                    Vulnerable; SMT Host state unknown\n",
            "  Meltdown:               Vulnerable\n",
            "  Mmio stale data:        Vulnerable\n",
            "  Reg file data sampling: Not affected\n",
            "  Retbleed:               Vulnerable\n",
            "  Spec rstack overflow:   Not affected\n",
            "  Spec store bypass:      Vulnerable\n",
            "  Spectre v1:             Vulnerable: __user pointer sanitization and usercopy b\n",
            "                          arriers only; no swapgs barriers\n",
            "  Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIB\n",
            "                          RS: Not affected; BHI: Vulnerable\n",
            "  Srbds:                  Not affected\n",
            "  Tsx async abort:        Vulnerable\n",
            "Thu Aug 14 05:59:36 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P0             26W /   70W |     154MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    }
  ]
}