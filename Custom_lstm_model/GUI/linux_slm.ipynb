{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Simple ui to intract with the lstm-model"
      ],
      "metadata": {
        "id": "5nNxhijDUt3M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install streamlit pyngrok nltk\n",
        "\n",
        "# Download NLTK data if needed\n",
        "import nltk\n",
        "nltk.download('punkt', quiet=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ucp8s9LRUs14",
        "outputId": "0176f877-0843-4ace-db96-0ee6fbac8a58"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.49.1-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.3.0-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.2.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2025.8.3)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.27.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading streamlit-1.49.1-py3-none-any.whl (10.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m93.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyngrok-7.3.0-py3-none-any.whl (25 kB)\n",
            "Downloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m122.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyngrok, pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 pyngrok-7.3.0 streamlit-1.49.1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the Streamlit app file\n",
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import re\n",
        "import pickle\n",
        "import os\n",
        "from collections import Counter\n",
        "from torch.amp import autocast\n",
        "\n",
        "# Custom dataset class for loading\n",
        "class StreamlitBookDataset:\n",
        "    def __init__(self, vocab_data):\n",
        "        self.vocab = vocab_data['vocab']\n",
        "        self.token_to_idx = vocab_data['token_to_idx']\n",
        "        self.idx_to_token = vocab_data['idx_to_token']\n",
        "        if 'seq_length' in vocab_data:\n",
        "            self.seq_length = vocab_data['seq_length']\n",
        "        else:\n",
        "            self.seq_length = 100\n",
        "\n",
        "    def tokenize_text(self, text):\n",
        "        # Same tokenization as training\n",
        "        tokens = re.findall(r'\\b\\w+\\b|[^\\w\\s]', text)\n",
        "        return [token.lower() for token in tokens if token.strip()]\n",
        "\n",
        "# Enhanced model class for loading\n",
        "class StreamlitTextGenerator(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=256, hidden_dim=512, num_layers=3, dropout=0.3):\n",
        "        super(StreamlitTextGenerator, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        self.embedding = torch.nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.lstm = torch.nn.LSTM(\n",
        "            embedding_dim,\n",
        "            hidden_dim,\n",
        "            num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0,\n",
        "            bidirectional=False\n",
        "        )\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "        self.layer_norm = torch.nn.LayerNorm(hidden_dim)\n",
        "        self.fc1 = torch.nn.Linear(hidden_dim, hidden_dim // 2)\n",
        "        self.fc2 = torch.nn.Linear(hidden_dim // 2, vocab_size)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        embedded = self.embedding(x)\n",
        "        lstm_out, hidden = self.lstm(embedded, hidden)\n",
        "        lstm_out = self.layer_norm(lstm_out)\n",
        "        lstm_out = self.dropout(lstm_out)\n",
        "        out = self.relu(self.fc1(lstm_out))\n",
        "        out = self.dropout(out)\n",
        "        output = self.fc2(out)\n",
        "        return output, hidden\n",
        "\n",
        "def load_model_and_dataset(model_path, dataset_path):\n",
        "    \"\"\"Load trained model and dataset\"\"\"\n",
        "    try:\n",
        "        # Load dataset info\n",
        "        with open(dataset_path, 'rb') as f:\n",
        "            dataset_info = pickle.load(f)\n",
        "\n",
        "        dataset = StreamlitBookDataset(dataset_info)\n",
        "\n",
        "        # Load model\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        checkpoint = torch.load(model_path, map_location=device)\n",
        "\n",
        "        model = StreamlitTextGenerator(\n",
        "            vocab_size=len(dataset.vocab),\n",
        "            embedding_dim=256,\n",
        "            hidden_dim=512,\n",
        "            num_layers=3\n",
        "        )\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        model.to(device)\n",
        "        model.eval()\n",
        "\n",
        "        return model, dataset, device\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error loading model: {str(e)}\")\n",
        "        return None, None, None\n",
        "\n",
        "def intelligent_generate_text(model, dataset, seed_text, max_length=150, temperature=0.8, top_k=50):\n",
        "    \"\"\"Generate text using the trained model\"\"\"\n",
        "    if model is None or dataset is None:\n",
        "        return \"Model not loaded properly\"\n",
        "\n",
        "    device = next(model.parameters()).device\n",
        "    model.eval()\n",
        "\n",
        "    try:\n",
        "        # Tokenize seed text\n",
        "        seed_tokens = dataset.tokenize_text(seed_text)\n",
        "        if not seed_tokens:\n",
        "            seed_tokens = ['the']\n",
        "\n",
        "        seed_indices = [dataset.token_to_idx.get(token, dataset.token_to_idx.get('<UNK>', 0)) for token in seed_tokens]\n",
        "\n",
        "        # Convert to tensor\n",
        "        input_seq = torch.tensor([seed_indices], dtype=torch.long).to(device)\n",
        "        generated_tokens = seed_tokens.copy()\n",
        "\n",
        "        hidden = None\n",
        "        with torch.no_grad():\n",
        "            for _ in range(max_length):\n",
        "                # Get prediction\n",
        "                with autocast('cuda' if device.type == 'cuda' else 'cpu'):\n",
        "                    output, hidden = model(input_seq, hidden)\n",
        "\n",
        "                # Get logits for last token\n",
        "                logits = output[0, -1] / temperature\n",
        "\n",
        "                # Top-k sampling\n",
        "                top_logits, top_indices = torch.topk(logits, min(top_k, len(logits)))\n",
        "                probabilities = F.softmax(top_logits, dim=-1)\n",
        "\n",
        "                # Sample from top-k\n",
        "                try:\n",
        "                    next_token_idx_local = torch.multinomial(probabilities, 1).item()\n",
        "                    next_token_idx = top_indices[next_token_idx_local].item()\n",
        "                except:\n",
        "                    # Fallback to argmax if sampling fails\n",
        "                    next_token_idx = torch.argmax(logits).item()\n",
        "\n",
        "                # Convert back to token\n",
        "                if next_token_idx < len(dataset.idx_to_token):\n",
        "                    next_token = dataset.idx_to_token[next_token_idx]\n",
        "                    if next_token not in ['<PAD>', '<UNK>']:\n",
        "                        generated_tokens.append(next_token)\n",
        "                else:\n",
        "                    generated_tokens.append('<UNK>')\n",
        "\n",
        "                # Update input sequence\n",
        "                new_input = torch.tensor([[next_token_idx]], device=device)\n",
        "                input_seq = torch.cat([input_seq, new_input], dim=1)\n",
        "\n",
        "                # Keep reasonable context length\n",
        "                if input_seq.size(1) > 200:\n",
        "                    input_seq = input_seq[:, -200:]\n",
        "\n",
        "        return ' '.join(generated_tokens)\n",
        "    except Exception as e:\n",
        "        return f\"Error in generation: {str(e)}\"\n",
        "\n",
        "def format_answer(text, question):\n",
        "    \"\"\"Format the generated answer to be more relevant\"\"\"\n",
        "    # Simple formatting - you can make this more sophisticated\n",
        "    sentences = text.split('.')\n",
        "    if len(sentences) > 3:\n",
        "        # Return first 2-3 sentences\n",
        "        return '. '.join(sentences[:3]) + '.'\n",
        "    return text\n",
        "\n",
        "# Streamlit UI\n",
        "def main():\n",
        "    st.set_page_config(\n",
        "        page_title=\"Book Knowledge Assistant\",\n",
        "        page_icon=\"📚\",\n",
        "        layout=\"wide\"\n",
        "    )\n",
        "\n",
        "    st.title(\"📚 Book Knowledge Assistant\")\n",
        "    st.markdown(\"Ask questions about your trained book content!\")\n",
        "\n",
        "    # Initialize session state\n",
        "    if 'model_loaded' not in st.session_state:\n",
        "        st.session_state.model_loaded = False\n",
        "        st.session_state.model = None\n",
        "        st.session_state.dataset = None\n",
        "        st.session_state.device = None\n",
        "\n",
        "    # Sidebar for model loading\n",
        "    with st.sidebar:\n",
        "        st.header(\"Model Configuration\")\n",
        "\n",
        "        model_file = st.file_uploader(\"Upload Model File (.pth)\", type=['pth'])\n",
        "        dataset_file = st.file_uploader(\"Upload Dataset File (.pkl)\", type=['pkl'])\n",
        "\n",
        "        if st.button(\"Load Model\") and model_file and dataset_file:\n",
        "            with st.spinner(\"Loading model...\"):\n",
        "                try:\n",
        "                    # Save uploaded files temporarily\n",
        "                    with open(\"/tmp/model.pth\", \"wb\") as f:\n",
        "                        f.write(model_file.getvalue())\n",
        "                    with open(\"/tmp/dataset.pkl\", \"wb\") as f:\n",
        "                        f.write(dataset_file.getvalue())\n",
        "\n",
        "                    # Load model and dataset\n",
        "                    model, dataset, device = load_model_and_dataset(\"/tmp/model.pth\", \"/tmp/dataset.pkl\")\n",
        "\n",
        "                    if model is not None:\n",
        "                        st.session_state.model_loaded = True\n",
        "                        st.session_state.model = model\n",
        "                        st.session_state.dataset = dataset\n",
        "                        st.session_state.device = device\n",
        "                        st.success(\"Model loaded successfully!\")\n",
        "                    else:\n",
        "                        st.error(\"Failed to load model\")\n",
        "                except Exception as e:\n",
        "                    st.error(f\"Error: {str(e)}\")\n",
        "\n",
        "        if st.session_state.model_loaded:\n",
        "            st.success(\"✅ Model Ready\")\n",
        "            st.info(f\"Device: {st.session_state.device}\")\n",
        "            st.info(f\"Vocabulary: {len(st.session_state.dataset.vocab)} tokens\")\n",
        "\n",
        "    # Main interface\n",
        "    if not st.session_state.model_loaded:\n",
        "        st.info(\"👈 Please upload and load your model files in the sidebar\")\n",
        "        st.markdown(\"\"\"\n",
        "        ### How to use:\n",
        "        1. Upload your trained model file (.pth)\n",
        "        2. Upload your dataset file (.pkl)\n",
        "        3. Click 'Load Model'\n",
        "        4. Ask questions in the main panel\n",
        "        \"\"\")\n",
        "        return\n",
        "\n",
        "    # Question interface\n",
        "    st.header(\"Ask Your Question\")\n",
        "\n",
        "    # Predefined questions\n",
        "    predefined_questions = [\n",
        "        \"What are the main concepts discussed?\",\n",
        "        \"Explain the key features\",\n",
        "        \"How does this work?\",\n",
        "        \"What are the benefits?\",\n",
        "        \"Tell me more about this topic\"\n",
        "    ]\n",
        "\n",
        "    selected_question = st.selectbox(\n",
        "        \"Choose a predefined question or type your own:\",\n",
        "        [\"\"] + predefined_questions,\n",
        "        key=\"predefined_q\"\n",
        "    )\n",
        "\n",
        "    user_question = st.text_input(\n",
        "        \"Your Question:\",\n",
        "        value=selected_question if selected_question else \"\",\n",
        "        placeholder=\"Enter your question about the book content...\"\n",
        "    )\n",
        "\n",
        "    # Generation parameters\n",
        "    col1, col2, col3 = st.columns(3)\n",
        "    with col1:\n",
        "        max_length = st.slider(\"Response Length\", 50, 300, 150)\n",
        "    with col2:\n",
        "        temperature = st.slider(\"Creativity\", 0.1, 1.5, 0.8)\n",
        "    with col3:\n",
        "        top_k = st.slider(\"Diversity (Top-K)\", 10, 100, 50)\n",
        "\n",
        "    if st.button(\"📝 Generate Answer\", type=\"primary\") and user_question:\n",
        "        with st.spinner(\"Generating answer...\"):\n",
        "            try:\n",
        "                # Generate response\n",
        "                prompt = f\"Question: {user_question} Answer:\"\n",
        "                generated_text = intelligent_generate_text(\n",
        "                    st.session_state.model,\n",
        "                    st.session_state.dataset,\n",
        "                    prompt,\n",
        "                    max_length=max_length,\n",
        "                    temperature=temperature,\n",
        "                    top_k=top_k\n",
        "                )\n",
        "\n",
        "                # Format and display answer\n",
        "                formatted_answer = format_answer(generated_text, user_question)\n",
        "\n",
        "                st.subheader(\"🤖 Generated Answer:\")\n",
        "                st.markdown(f\"**{formatted_answer}**\")\n",
        "\n",
        "                # Show raw generation for debugging\n",
        "                with st.expander(\"🔍 See raw generation\"):\n",
        "                    st.text(generated_text)\n",
        "\n",
        "            except Exception as e:\n",
        "                st.error(f\"Error generating answer: {str(e)}\")\n",
        "\n",
        "    # Example usage\n",
        "    st.markdown(\"---\")\n",
        "    st.markdown(\"### 💡 Tips:\")\n",
        "    st.markdown(\"\"\"\n",
        "    - Be specific with your questions\n",
        "    - Try different creativity levels\n",
        "    - Longer responses may take more time\n",
        "    - The model works best with questions related to your book's content\n",
        "    \"\"\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNEUUoIgUwmu",
        "outputId": "fdb2b640-0fa2-4418-a2f2-99f3e5e0f43f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Put ur ngrok Token"
      ],
      "metadata": {
        "id": "z4_Qp4qbcSig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok config add-authtoken 31CCfPaNlvTntxLilA1mz9MSP38_6YQUypMVfgWAe2SspGUz8"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8ux39XrS6I3",
        "outputId": "e6e67054-7df7-4d4d-f879-7f5c657dd640"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the Streamlit app with Ngrok tunnel\n",
        "from pyngrok import ngrok\n",
        "import streamlit as st\n",
        "import subprocess\n",
        "import threading\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Kill any existing ngrok processes\n",
        "!pkill ngrok\n",
        "\n",
        "# Set up Ngrok\n",
        "ngrok.set_auth_token(\"31CCfPaNlvTntxLilA1mz9MSP38_6YQUypMVfgWAe2SspGUz8\")  # Add your Ngrok auth token here if you have one\n",
        "\n",
        "# Function to run Streamlit\n",
        "def run_streamlit():\n",
        "    os.system(\"streamlit run app.py --server.port 8501 --server.address 0.0.0.0\")\n",
        "\n",
        "# Start Streamlit in background thread\n",
        "streamlit_thread = threading.Thread(target=run_streamlit)\n",
        "streamlit_thread.daemon = True\n",
        "streamlit_thread.start()\n",
        "\n",
        "# Wait a moment for Streamlit to start\n",
        "time.sleep(5)\n",
        "\n",
        "# Create Ngrok tunnel\n",
        "try:\n",
        "    public_url = ngrok.connect(8501)\n",
        "    print(\"✅ Streamlit app is running!\")\n",
        "    print(f\"🔗 Public URL: {public_url}\")\n",
        "    print(\"📝 To use the app:\")\n",
        "    print(\"   1. Click the URL above\")\n",
        "    print(\"   2. Upload your model (.pth) and dataset (.pkl) files\")\n",
        "    print(\"   3. Ask questions about your book content\")\n",
        "    print(\"\\n⚠️  Keep this cell running to maintain the web interface!\")\n",
        "\n",
        "    # Display QR code for easy access (optional)\n",
        "    try:\n",
        "        import qrcode\n",
        "        from PIL import Image\n",
        "        qr = qrcode.QRCode(version=1, box_size=10, border=5)\n",
        "        qr.add_data(str(public_url))\n",
        "        qr.make(fit=True)\n",
        "        img = qr.make_image(fill_color=\"black\", back_color=\"white\")\n",
        "        img.save(\"/content/qr_code.png\")\n",
        "        print(\"📱 QR code saved as 'qr_code.png'\")\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error creating Ngrok tunnel: {e}\")\n",
        "    print(\"🔧 Trying alternative method...\")\n",
        "\n",
        "    # Alternative: Display local URL\n",
        "    print(\"🌐 Access the app locally at: http://localhost:8501\")\n",
        "    print(\"💡 If you want public access, get a free Ngrok account at https://ngrok.com\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVa_Sd1oU4lx",
        "outputId": "1726783b-6e51-488b-b694-f2c7dd62192f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:pyngrok.process.ngrok:t=2025-08-30T05:28:09+0000 lvl=eror msg=\"failed to reconnect session\" obj=tunnels.session err=\"authentication failed: The account 'Erehn' has been suspended.\\nThis is usually the result of violating the ngrok Terms of Service.\\nEmail support@ngrok.com if you think your suspension is an error.\\r\\n\\r\\nERR_NGROK_103\\r\\n\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ Error creating Ngrok tunnel: The ngrok process errored on start: authentication failed: The account 'Erehn' has been suspended.\\nThis is usually the result of violating the ngrok Terms of Service.\\nEmail support@ngrok.com if you think your suspension is an error.\\r\\n\\r\\nERR_NGROK_103\\r\\n.\n",
            "🔧 Trying alternative method...\n",
            "🌐 Access the app locally at: http://localhost:8501\n",
            "💡 If you want public access, get a free Ngrok account at https://ngrok.com\n"
          ]
        }
      ]
    }
  ]
}